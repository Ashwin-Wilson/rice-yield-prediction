{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa4750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-19 14:21:58.113245: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-19 14:21:58.120668: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-19 14:21:58.171270: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-19 14:21:58.218659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752915118.264667    6315 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752915118.277921    6315 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752915118.365498    6315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752915118.365518    6315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752915118.365520    6315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752915118.365520    6315 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-19 14:21:58.375439: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbb5287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province</th>\n",
       "      <th>Harvested Area</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aceh</td>\n",
       "      <td>329516</td>\n",
       "      <td>2336</td>\n",
       "      <td>81</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aceh</td>\n",
       "      <td>310012</td>\n",
       "      <td>1437</td>\n",
       "      <td>82</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aceh</td>\n",
       "      <td>317869</td>\n",
       "      <td>1790</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aceh</td>\n",
       "      <td>297058</td>\n",
       "      <td>2293</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aceh</td>\n",
       "      <td>271750</td>\n",
       "      <td>1834</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Papua</td>\n",
       "      <td>54132</td>\n",
       "      <td>1823</td>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Papua</td>\n",
       "      <td>52728</td>\n",
       "      <td>1502</td>\n",
       "      <td>75</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Papua</td>\n",
       "      <td>64985</td>\n",
       "      <td>2028</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Papua</td>\n",
       "      <td>49742</td>\n",
       "      <td>2576</td>\n",
       "      <td>84</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Papua</td>\n",
       "      <td>49323</td>\n",
       "      <td>2555</td>\n",
       "      <td>80</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Province  Harvested Area  Rainfall  Humidity  Temperature\n",
       "0       Aceh          329516      2336        81           28\n",
       "1       Aceh          310012      1437        82           27\n",
       "2       Aceh          317869      1790        76           29\n",
       "3       Aceh          297058      2293        76           29\n",
       "4       Aceh          271750      1834        76           29\n",
       "..       ...             ...       ...       ...          ...\n",
       "199    Papua           54132      1823        77           28\n",
       "200    Papua           52728      1502        75           28\n",
       "201    Papua           64985      2028        76           28\n",
       "202    Papua           49742      2576        84           28\n",
       "203    Papua           49323      2555        80           28\n",
       "\n",
       "[204 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Loading, Preparing and Scaling\n",
    "df = pd.read_csv(\"./dataset.csv\")\n",
    "input_n = df.drop(['Production','Year'], axis='columns')\n",
    "input_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd0ec2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1861567\n",
       "1      1714438\n",
       "2      1757313\n",
       "3      1634640\n",
       "4      1509456\n",
       "        ...   \n",
       "199     235340\n",
       "200     166002\n",
       "201     286280\n",
       "202     193944\n",
       "203     200115\n",
       "Name: Production, Length: 204, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['Production']\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb8de7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Province</th>\n",
       "      <th>Harvested Area</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>329516</td>\n",
       "      <td>2336</td>\n",
       "      <td>81</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>310012</td>\n",
       "      <td>1437</td>\n",
       "      <td>82</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>317869</td>\n",
       "      <td>1790</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>297058</td>\n",
       "      <td>2293</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>271750</td>\n",
       "      <td>1834</td>\n",
       "      <td>76</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>23</td>\n",
       "      <td>54132</td>\n",
       "      <td>1823</td>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>23</td>\n",
       "      <td>52728</td>\n",
       "      <td>1502</td>\n",
       "      <td>75</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>23</td>\n",
       "      <td>64985</td>\n",
       "      <td>2028</td>\n",
       "      <td>76</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>23</td>\n",
       "      <td>49742</td>\n",
       "      <td>2576</td>\n",
       "      <td>84</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>23</td>\n",
       "      <td>49323</td>\n",
       "      <td>2555</td>\n",
       "      <td>80</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Province  Harvested Area  Rainfall  Humidity  Temperature\n",
       "0           0          329516      2336        81           28\n",
       "1           0          310012      1437        82           27\n",
       "2           0          317869      1790        76           29\n",
       "3           0          297058      2293        76           29\n",
       "4           0          271750      1834        76           29\n",
       "..        ...             ...       ...       ...          ...\n",
       "199        23           54132      1823        77           28\n",
       "200        23           52728      1502        75           28\n",
       "201        23           64985      2028        76           28\n",
       "202        23           49742      2576        84           28\n",
       "203        23           49323      2555        80           28\n",
       "\n",
       "[204 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_province = LabelEncoder()\n",
    "input_n['Province'] = le_province.fit_transform(input_n['Province'])\n",
    "input_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb5b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Province  Harvested Area  Rainfall  Humidity  Temperature\n",
      "199  0.696970        0.029636  0.276728  0.363636          0.6\n",
      "93   0.060606        0.174608  0.516504  0.272727          0.8\n",
      "38   0.090909        0.035128  0.785344  0.363636          0.8\n",
      "24   0.212121        0.047240  0.446959  0.727273          0.4\n",
      "96   0.030303        0.060839  0.222960  0.500000          0.6\n",
      "..        ...             ...       ...       ...          ...\n",
      "106  0.636364        0.148176  0.448412  0.409091          0.8\n",
      "14   0.939394        0.162212  0.880424  1.000000          0.6\n",
      "92   0.060606        0.178497  0.458377  0.227273          1.0\n",
      "179  0.787879        0.032372  0.428690  0.500000          0.6\n",
      "102  0.636364        0.158688  0.244551  0.227273          0.8\n",
      "\n",
      "[163 rows x 5 columns]\n",
      "[2.23748270e-02 1.52662045e-01 2.78508815e-02 3.64431838e-02\n",
      " 6.34951446e-02 6.96679212e-02 2.61100764e-02 2.19494598e-02\n",
      " 4.26319617e-04 2.85469368e-02 2.31739381e-02 1.60713733e-01\n",
      " 1.32682075e-01 4.94595523e-02 4.25186193e-03 2.81174741e-03\n",
      " 2.27865740e-03 2.29189655e-03 4.70768199e-01 9.19618941e-01\n",
      " 3.36302935e-02 5.06464085e-02 2.47921906e-01 1.41216278e-01\n",
      " 2.63033965e-01 4.22069755e-02 2.36992085e-01 9.16095137e-01\n",
      " 2.22731998e-03 6.55280682e-02 6.67414980e-03 2.41347764e-02\n",
      " 5.04426781e-02 2.63722877e-02 8.07161331e-02 7.61291017e-02\n",
      " 3.67638760e-02 7.80058224e-02 6.36421087e-02 7.19701995e-02\n",
      " 1.67336164e-01 8.63048823e-01 2.55995310e-01 2.49877966e-02\n",
      " 8.82504466e-02 9.12502185e-01 2.27639055e-02 2.74677081e-02\n",
      " 7.72729069e-02 2.79641286e-04 2.03000905e-02 4.84820841e-01\n",
      " 2.52439123e-03 2.43068854e-01 1.98133852e-01 2.12108296e-02\n",
      " 2.21409988e-02 2.00764727e-01 2.83571122e-02 2.28161001e-03\n",
      " 1.43728858e-01 2.61224297e-01 1.25416355e-01 2.56322288e-02\n",
      " 2.22846293e-02 5.12853165e-02 1.98888293e-01 3.92507404e-04\n",
      " 7.21087820e-02 4.63122544e-03 2.28315299e-02 1.77265906e-01\n",
      " 1.10847863e-02 2.36643581e-02 8.98480974e-01 1.09521471e-01\n",
      " 1.82395457e-04 2.52388357e-01 2.04652465e-02 2.64136243e-02\n",
      " 2.67818441e-02 8.56747179e-02 4.35776559e-02 4.48420612e-01\n",
      " 3.13682088e-03 5.07040322e-02 2.94794872e-02 1.98841527e-02\n",
      " 2.19815576e-02 2.64166722e-02 5.29668788e-02 2.57824313e-02\n",
      " 4.55783865e-02 2.59864284e-01 1.70314496e-01 1.04793095e-02\n",
      " 2.06079817e-01 7.95167044e-02 1.55652092e-01 1.35166654e-01\n",
      " 6.19839768e-03 2.81393806e-03 8.77955533e-03 2.31227912e-02\n",
      " 2.63144736e-03 4.60827123e-03 4.98109135e-02 2.64273016e-01\n",
      " 1.97966124e-01 6.90151074e-02 1.40018754e-01 5.07070800e-02\n",
      " 1.10300200e-02 7.87129262e-03 9.13501121e-01 1.94308500e-01\n",
      " 1.41208658e-01 0.00000000e+00 7.73894876e-02 9.67592185e-02\n",
      " 1.38780465e-01 1.00000000e+00 1.84320372e-02 6.96058210e-02\n",
      " 5.10492596e-01 3.57390326e-03 2.69164262e-04 6.41955813e-05\n",
      " 1.47287618e-01 5.41957384e-03 1.57706827e-02 7.08614447e-02\n",
      " 8.00063624e-06 4.31481932e-03 9.07319106e-01 2.06717391e-02\n",
      " 4.11461292e-05 2.16401971e-02 2.24217831e-03 6.77649127e-02\n",
      " 2.81973852e-02 7.54179975e-02 2.33770971e-03 1.63252506e-01\n",
      " 5.81017633e-03 3.01028701e-02 3.62664079e-02 8.04331583e-02\n",
      " 1.33511474e-01 5.89083037e-02 1.26397575e-01 9.32375575e-01\n",
      " 9.03761585e-01 1.27861501e-01 1.90197982e-02 2.31696521e-02\n",
      " 4.09175396e-03 8.66308416e-01 1.38346430e-01 1.32091076e-01\n",
      " 1.57607486e-01 2.79645096e-02 1.39050677e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    input_n,  # your feature dataframe\n",
    "    target,    # your target array/series\n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#scaling feature\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "#scale features\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=[\"Province\",\"Harvested Area\",\"Rainfall\",\"Humidity\",\"Temperature\"], index=X_train.index)\n",
    "\n",
    "# Scale target variable\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "print(X_train_scaled)\n",
    "print(y_train_scaled)\n",
    "\n",
    "#Scale test data\n",
    "X_test_scaled = feature_scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=[\"Province\",\"Harvested Area\",\"Rainfall\",\"Humidity\",\"Temperature\"], index=X_test.index)\n",
    "y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5da2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1752915119.972862    6315 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1752915119.976519    6315 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - loss: 0.0536 - val_loss: 0.1288\n",
      "Epoch 2/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0401 - val_loss: 0.1123\n",
      "Epoch 3/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0527 - val_loss: 0.1007\n",
      "Epoch 4/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0379 - val_loss: 0.0967\n",
      "Epoch 5/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0425 - val_loss: 0.0938\n",
      "Epoch 6/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0450 - val_loss: 0.0898\n",
      "Epoch 7/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0418 - val_loss: 0.0850\n",
      "Epoch 8/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0320 - val_loss: 0.0777\n",
      "Epoch 9/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0282 - val_loss: 0.0669\n",
      "Epoch 10/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0282 - val_loss: 0.0538\n",
      "Epoch 11/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0311 - val_loss: 0.0440\n",
      "Epoch 12/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0222 - val_loss: 0.0372\n",
      "Epoch 13/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0166 - val_loss: 0.0260\n",
      "Epoch 14/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0154 - val_loss: 0.0147\n",
      "Epoch 15/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0167 - val_loss: 0.0086\n",
      "Epoch 16/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0126 - val_loss: 0.0053\n",
      "Epoch 17/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0080 - val_loss: 0.0027\n",
      "Epoch 18/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0128 - val_loss: 0.0013\n",
      "Epoch 19/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0086 - val_loss: 0.0017\n",
      "Epoch 20/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0117 - val_loss: 0.0027\n",
      "Epoch 21/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0127 - val_loss: 0.0018\n",
      "Epoch 22/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097 - val_loss: 0.0036\n",
      "Epoch 23/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0090 - val_loss: 0.0028\n",
      "Epoch 24/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0086 - val_loss: 0.0029\n",
      "Epoch 25/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087 - val_loss: 0.0042\n",
      "Epoch 26/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0101 - val_loss: 0.0038\n",
      "Epoch 27/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097 - val_loss: 0.0025\n",
      "Epoch 28/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087 - val_loss: 0.0029\n",
      "Epoch 29/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0103 - val_loss: 0.0046\n",
      "Epoch 30/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0101 - val_loss: 0.0026\n",
      "Epoch 31/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0072 - val_loss: 0.0030\n",
      "Epoch 32/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0093 - val_loss: 0.0048\n",
      "Epoch 33/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0096 - val_loss: 0.0041\n",
      "Epoch 34/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088 - val_loss: 0.0025\n",
      "Epoch 35/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0084 - val_loss: 0.0030\n",
      "Epoch 36/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0094 - val_loss: 0.0037\n",
      "Epoch 37/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0113 - val_loss: 0.0032\n",
      "Epoch 38/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0111 - val_loss: 0.0022\n",
      "Epoch 39/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0072 - val_loss: 0.0027\n",
      "Epoch 40/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0132 - val_loss: 0.0035\n",
      "Epoch 41/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0106 - val_loss: 0.0032\n",
      "Epoch 42/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0082 - val_loss: 0.0022\n",
      "Epoch 43/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0112 - val_loss: 0.0027\n",
      "Epoch 44/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0078 - val_loss: 0.0032\n",
      "Epoch 45/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0096 - val_loss: 0.0027\n",
      "Epoch 46/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0021\n",
      "Epoch 47/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0095 - val_loss: 0.0024\n",
      "Epoch 48/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0108 - val_loss: 0.0026\n",
      "Epoch 49/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0076 - val_loss: 0.0028\n",
      "Epoch 50/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0089 - val_loss: 0.0022\n",
      "Epoch 51/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0088 - val_loss: 0.0022\n",
      "Epoch 52/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0099 - val_loss: 0.0024\n",
      "Epoch 53/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0088 - val_loss: 0.0030\n",
      "Epoch 54/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0080 - val_loss: 0.0029\n",
      "Epoch 55/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0074 - val_loss: 0.0021\n",
      "Epoch 56/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0093 - val_loss: 0.0027\n",
      "Epoch 57/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0072 - val_loss: 0.0038\n",
      "Epoch 58/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0078 - val_loss: 0.0033\n",
      "Epoch 59/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097 - val_loss: 0.0032\n",
      "Epoch 60/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0087 - val_loss: 0.0028\n",
      "Epoch 61/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0072 - val_loss: 0.0028\n",
      "Epoch 62/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0070 - val_loss: 0.0021\n",
      "Epoch 63/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0074 - val_loss: 0.0023\n",
      "Epoch 64/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0069 - val_loss: 0.0020\n",
      "Epoch 65/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0025\n",
      "Epoch 66/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0019\n",
      "Epoch 67/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0071 - val_loss: 0.0024\n",
      "Epoch 68/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0109 - val_loss: 0.0033\n",
      "Epoch 69/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0073 - val_loss: 0.0029\n",
      "Epoch 70/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0097 - val_loss: 0.0019\n",
      "Epoch 71/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0093 - val_loss: 0.0021\n",
      "Epoch 72/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0120 - val_loss: 0.0035\n",
      "Epoch 73/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0074 - val_loss: 0.0029\n",
      "Epoch 74/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0090 - val_loss: 0.0017\n",
      "Epoch 75/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0113 - val_loss: 0.0019\n",
      "Epoch 76/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0102 - val_loss: 0.0049\n",
      "Epoch 77/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0120 - val_loss: 0.0025\n",
      "Epoch 78/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0088 - val_loss: 0.0016\n",
      "Epoch 79/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0092 - val_loss: 0.0028\n",
      "Epoch 80/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0081 - val_loss: 0.0033\n",
      "Epoch 81/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0095 - val_loss: 0.0019\n",
      "Epoch 82/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0078 - val_loss: 0.0024\n",
      "Epoch 83/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0103 - val_loss: 0.0032\n",
      "Epoch 84/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0067 - val_loss: 0.0020\n",
      "Epoch 85/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0106 - val_loss: 0.0016\n",
      "Epoch 86/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0093 - val_loss: 0.0021\n",
      "Epoch 87/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0052 - val_loss: 0.0032\n",
      "Epoch 88/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 89/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0019\n",
      "Epoch 90/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0106 - val_loss: 0.0019\n",
      "Epoch 91/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0083 - val_loss: 0.0030\n",
      "Epoch 92/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0091 - val_loss: 0.0024\n",
      "Epoch 93/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0069 - val_loss: 0.0019\n",
      "Epoch 94/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0078 - val_loss: 0.0029\n",
      "Epoch 95/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0107 - val_loss: 0.0034\n",
      "Epoch 96/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0089 - val_loss: 0.0020\n",
      "Epoch 97/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0080 - val_loss: 0.0018\n",
      "Epoch 98/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 99/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0085 - val_loss: 0.0028\n",
      "Epoch 100/100\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0091 - val_loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "X_train_reshaped = X_train_scaled.values.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.values.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Update your model's input_shape\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, return_sequences=True, input_shape=(1, 5)))  # (timesteps, features)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train with reshaped data\n",
    "history = model.fit(X_train_reshaped, y_train_scaled, epochs=100, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "599a3c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
      "RMSE: 1340554.57\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test_reshaped)\n",
    "predictions = target_scaler.inverse_transform(predictions).flatten()\n",
    "y_test = target_scaler.inverse_transform(y_test_scaled.reshape(-1,1)).flatten()\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_test - predictions)**2))\n",
    "print(f'RMSE: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c6e440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "(41,)\n",
      "(41,)\n",
      "Mean Squared Error: 0.016303\n",
      "Mean Absolute Error: 0.072882\n",
      "R² Score: 0.848954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "y_pred_scaled = model.predict(X_test_reshaped)\n",
    "print(y_pred_scaled[:,0].shape)\n",
    "print(y_test_scaled.shape)\n",
    "\n",
    "# Regression metrics\n",
    "mse = mean_squared_error(y_test_scaled, y_pred_scaled[:,0])\n",
    "mae = mean_absolute_error(y_test_scaled, y_pred_scaled[:,0])\n",
    "r2 = r2_score(y_test_scaled, y_pred_scaled[:,0])\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.6f}\")\n",
    "print(f\"R² Score: {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5dc5f3",
   "metadata": {},
   "source": [
    "# Time Series Approach Using Year Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56107538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (204, 7)\n",
      "Years available: [np.int64(2018), np.int64(2019), np.int64(2020), np.int64(2021), np.int64(2022), np.int64(2023)]\n",
      "Provinces: ['Aceh' 'Sumatera Utara' 'Sumatera Barat' 'Riau' 'Jambi'\n",
      " 'Sumatera Selatan' 'Bengkulu' 'Lampung' 'Kepulauan Bangka Belitung'\n",
      " 'Kepulauan Riau' 'DKI Jakarta' 'Jawa Barat' 'Jawa Tengah' 'DI Yogyakarta'\n",
      " 'Jawa Timur' 'Banten' 'Bali' 'Nusa Tenggara Barat' 'Nusa Tenggara Timur'\n",
      " 'Kalimantan Barat' 'Kalimantan Tengah' 'Kalimantan Selatan'\n",
      " 'Kalimantan Timur' 'Kalimantan Utara' 'Sulawesi Utara' 'Sulawesi Tengah'\n",
      " 'Sulawesi Selatan' 'Sulawesi Tenggara' 'Gorontalo' 'Sulawesi Barat'\n",
      " 'Maluku' 'Maluku Utara' 'Papua Barat' 'Papua']\n",
      "\n",
      "Input features shape: (204, 6)\n",
      "\n",
      "Input features:\n",
      "   Province_encoded  Year  Harvested Area  Rainfall  Humidity  Temperature\n",
      "0                 0  2018          329516      2336        81           28\n",
      "1                 0  2019          310012      1437        82           27\n",
      "2                 0  2020          317869      1790        76           29\n",
      "3                 0  2021          297058      2293        76           29\n",
      "4                 0  2022          271750      1834        76           29\n",
      "5                 0  2023          254319      2555        80           28\n",
      "6                33  2018          408176      2388        79           29\n",
      "7                33  2019          413141      1884        84           27\n",
      "8                33  2020          388591      2741        83           29\n",
      "9                33  2021          385405      2543        77           29\n"
     ]
    }
   ],
   "source": [
    "# 1st cell: Load data and create input features for time series\n",
    "df_ts = pd.read_csv(\"./dataset.csv\")\n",
    "print(\"Dataset shape:\", df_ts.shape)\n",
    "print(\"Years available:\", sorted(df_ts['Year'].unique()))\n",
    "print(\"Provinces:\", df_ts['Province'].unique())\n",
    "\n",
    "# Encode provinces for time series\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_province_ts = LabelEncoder()\n",
    "df_ts['Province_encoded'] = le_province_ts.fit_transform(df_ts['Province'])\n",
    "\n",
    "# Create input features (excluding Production which is our target)\n",
    "input_n_ts = df_ts[['Province_encoded', 'Year', 'Harvested Area', 'Rainfall', 'Humidity', 'Temperature']]\n",
    "print(\"\\nInput features shape:\", input_n_ts.shape)\n",
    "print(\"\\nInput features:\")\n",
    "print(input_n_ts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d906ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target variable shape: (204,)\n",
      "\n",
      "Target variable (Production):\n",
      "0    1861567\n",
      "1    1714438\n",
      "2    1757313\n",
      "3    1634640\n",
      "4    1509456\n",
      "5    1393474\n",
      "6    2108285\n",
      "7    2078902\n",
      "8    2040500\n",
      "9    2004143\n",
      "Name: Production, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_ts = df_ts['Production']\n",
    "print(\"Target variable shape:\", target_ts.shape)\n",
    "print(\"\\nTarget variable (Production):\")\n",
    "print(target_ts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b88757bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: (102, 4, 5)\n",
      "Targets shape: (102,)\n",
      "\n",
      "Training data shape: (81, 4, 5)\n",
      "Training target shape: (81,)\n",
      "Test data shape: (21, 4, 5)\n",
      "Test target shape: (21,)\n"
     ]
    }
   ],
   "source": [
    "# 3rd cell: Structure data for time series, split and scale\n",
    "def create_time_series_sequences(data, target, sequence_length):\n",
    "    \"\"\"\n",
    "    Create time series sequences for LSTM\n",
    "    sequence_length: number of years to look back\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    # Group by province to create sequences within each province\n",
    "    provinces = data['Province_encoded'].unique()\n",
    "    \n",
    "    for province in provinces:\n",
    "        # Get data for this province, sorted by year\n",
    "        province_data = data[data['Province_encoded'] == province].sort_values('Year')\n",
    "        province_target = target[data['Province_encoded'] == province].reindex(province_data.index)\n",
    "        \n",
    "        # Create sequences for this province\n",
    "        for i in range(len(province_data) - sequence_length + 1):\n",
    "            # Get sequence of features (excluding Year as it's just for ordering)\n",
    "            seq_features = province_data.iloc[i:i+sequence_length][['Province_encoded', 'Harvested Area', 'Rainfall', 'Humidity', 'Temperature']].values\n",
    "            seq_target = province_target.iloc[i+sequence_length-1]  # Predict the last year in sequence\n",
    "            \n",
    "            sequences.append(seq_features)\n",
    "            targets.append(seq_target)\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create time series sequences (using 3 years to predict current year)\n",
    "X_sequences, y_sequences = create_time_series_sequences(input_n_ts, target_ts, 4)\n",
    "print(f\"Sequences shape: {X_sequences.shape}\")  # (samples, timesteps, features)\n",
    "print(f\"Targets shape: {y_sequences.shape}\")\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_ts, X_test_ts, y_train_ts, y_test_ts = train_test_split(\n",
    "    X_sequences,\n",
    "    y_sequences,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Scale features and target using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "feature_scaler_ts = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler_ts = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Reshape for scaling (combine all timesteps and samples for consistent scaling)\n",
    "X_train_reshaped_for_scaling = X_train_ts.reshape(-1, X_train_ts.shape[-1])\n",
    "\n",
    "# print(X_train_reshaped_for_scaling.shape)\n",
    "X_train_scaled_ts = feature_scaler_ts.fit_transform(X_train_reshaped_for_scaling)\n",
    "X_train_scaled_ts = X_train_scaled_ts.reshape(X_train_ts.shape)\n",
    "# Scale target\n",
    "y_train_scaled_ts = target_scaler_ts.fit_transform(y_train_ts.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Scale test data\n",
    "X_test_reshaped_for_scaling = X_test_ts.reshape(-1, X_test_ts.shape[-1])\n",
    "X_test_scaled_ts = feature_scaler_ts.transform(X_test_reshaped_for_scaling)\n",
    "X_test_scaled_ts = X_test_scaled_ts.reshape(X_test_ts.shape)\n",
    "y_test_scaled_ts = target_scaler_ts.transform(y_test_ts.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train_scaled_ts.shape}\")\n",
    "print(f\"Training target shape: {y_train_scaled_ts.shape}\")\n",
    "print(f\"Test data shape: {X_test_scaled_ts.shape}\")\n",
    "print(f\"Test target shape: {y_test_scaled_ts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09cee7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled_ts shape: (81, 4, 5)\n",
      "X_test_scaled_ts shape: (21, 4, 5)\n",
      "Model architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">68,608</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │        \u001b[38;5;34m68,608\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120,129</span> (469.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120,129\u001b[0m (469.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120,129</span> (469.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120,129\u001b[0m (469.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the model...\n",
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - loss: 0.1056 - mae: 0.1666 - val_loss: 0.0096 - val_mae: 0.0934\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0717 - mae: 0.1568 - val_loss: 0.0271 - val_mae: 0.1594\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0580 - mae: 0.1767 - val_loss: 0.0205 - val_mae: 0.1383\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0419 - mae: 0.1473 - val_loss: 0.0096 - val_mae: 0.0916\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0328 - mae: 0.1170 - val_loss: 0.0029 - val_mae: 0.0481\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0211 - mae: 0.0885 - val_loss: 0.0021 - val_mae: 0.0395\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0157 - mae: 0.0781 - val_loss: 0.0049 - val_mae: 0.0590\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0137 - mae: 0.0738 - val_loss: 0.0032 - val_mae: 0.0444\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0183 - mae: 0.0827 - val_loss: 0.0042 - val_mae: 0.0546\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0149 - mae: 0.0919 - val_loss: 0.0046 - val_mae: 0.0556\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0117 - mae: 0.0766 - val_loss: 0.0019 - val_mae: 0.0291\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0076 - mae: 0.0549 - val_loss: 0.0018 - val_mae: 0.0344\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0081 - mae: 0.0542 - val_loss: 0.0026 - val_mae: 0.0457\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0113 - mae: 0.0644 - val_loss: 0.0027 - val_mae: 0.0468\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0080 - mae: 0.0586 - val_loss: 0.0019 - val_mae: 0.0367\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0084 - mae: 0.0608 - val_loss: 0.0015 - val_mae: 0.0289\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0071 - mae: 0.0575 - val_loss: 0.0014 - val_mae: 0.0290\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0075 - mae: 0.0549 - val_loss: 0.0015 - val_mae: 0.0309\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0062 - mae: 0.0510 - val_loss: 0.0016 - val_mae: 0.0327\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0080 - mae: 0.0565 - val_loss: 0.0014 - val_mae: 0.0292\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0073 - mae: 0.0558 - val_loss: 0.0016 - val_mae: 0.0271\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0070 - mae: 0.0543 - val_loss: 0.0014 - val_mae: 0.0296\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0077 - mae: 0.0551 - val_loss: 0.0020 - val_mae: 0.0381\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0081 - mae: 0.0561 - val_loss: 0.0025 - val_mae: 0.0432\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0064 - mae: 0.0548 - val_loss: 0.0018 - val_mae: 0.0357\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0062 - mae: 0.0548 - val_loss: 0.0014 - val_mae: 0.0297\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0055 - mae: 0.0518 - val_loss: 0.0015 - val_mae: 0.0273\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0077 - mae: 0.0593 - val_loss: 0.0014 - val_mae: 0.0287\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0066 - mae: 0.0561 - val_loss: 0.0019 - val_mae: 0.0374\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0058 - mae: 0.0553 - val_loss: 0.0021 - val_mae: 0.0394\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0062 - mae: 0.0540 - val_loss: 0.0016 - val_mae: 0.0329\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0060 - mae: 0.0523 - val_loss: 0.0015 - val_mae: 0.0312\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0072 - mae: 0.0585 - val_loss: 0.0019 - val_mae: 0.0375\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0056 - mae: 0.0535 - val_loss: 0.0025 - val_mae: 0.0446\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0059 - mae: 0.0549 - val_loss: 0.0020 - val_mae: 0.0387\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0054 - mae: 0.0546 - val_loss: 0.0016 - val_mae: 0.0342\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0044 - mae: 0.0501 - val_loss: 0.0018 - val_mae: 0.0375\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0053 - mae: 0.0542 - val_loss: 0.0021 - val_mae: 0.0414\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0054 - mae: 0.0591 - val_loss: 0.0019 - val_mae: 0.0389\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0058 - mae: 0.0560 - val_loss: 0.0015 - val_mae: 0.0332\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0058 - mae: 0.0557 - val_loss: 0.0015 - val_mae: 0.0321\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0055 - mae: 0.0542 - val_loss: 0.0014 - val_mae: 0.0317\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0048 - mae: 0.0520 - val_loss: 0.0014 - val_mae: 0.0315\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0058 - mae: 0.0560 - val_loss: 0.0013 - val_mae: 0.0287\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0045 - mae: 0.0492 - val_loss: 0.0012 - val_mae: 0.0273\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0049 - mae: 0.0502 - val_loss: 0.0014 - val_mae: 0.0329\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0050 - mae: 0.0515 - val_loss: 0.0015 - val_mae: 0.0345\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0060 - mae: 0.0556 - val_loss: 0.0016 - val_mae: 0.0351\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0047 - mae: 0.0544 - val_loss: 0.0015 - val_mae: 0.0336\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0042 - mae: 0.0493 - val_loss: 0.0013 - val_mae: 0.0294\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0061 - mae: 0.0557 - val_loss: 0.0013 - val_mae: 0.0297\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0049 - mae: 0.0509 - val_loss: 0.0013 - val_mae: 0.0305\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0049 - mae: 0.0507 - val_loss: 0.0012 - val_mae: 0.0277\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0054 - mae: 0.0519 - val_loss: 0.0013 - val_mae: 0.0274\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0054 - mae: 0.0508 - val_loss: 0.0012 - val_mae: 0.0260\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0052 - mae: 0.0562 - val_loss: 0.0020 - val_mae: 0.0407\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0049 - mae: 0.0560 - val_loss: 0.0026 - val_mae: 0.0472\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0044 - mae: 0.0537 - val_loss: 0.0016 - val_mae: 0.0361\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0045 - mae: 0.0500 - val_loss: 0.0012 - val_mae: 0.0265\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0046 - mae: 0.0486 - val_loss: 0.0012 - val_mae: 0.0264\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0047 - mae: 0.0499 - val_loss: 0.0015 - val_mae: 0.0350\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0046 - mae: 0.0527 - val_loss: 0.0016 - val_mae: 0.0376\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0044 - mae: 0.0529 - val_loss: 0.0012 - val_mae: 0.0307\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0059 - mae: 0.0570 - val_loss: 0.0011 - val_mae: 0.0273\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0040 - mae: 0.0473 - val_loss: 0.0012 - val_mae: 0.0273\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0064 - mae: 0.0574 - val_loss: 0.0011 - val_mae: 0.0240\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0047 - mae: 0.0502 - val_loss: 0.0012 - val_mae: 0.0296\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0051 - mae: 0.0530 - val_loss: 0.0020 - val_mae: 0.0412\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0045 - mae: 0.0552 - val_loss: 0.0030 - val_mae: 0.0507\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0057 - mae: 0.0623 - val_loss: 0.0019 - val_mae: 0.0401\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0041 - mae: 0.0494 - val_loss: 0.0011 - val_mae: 0.0276\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0090 - mae: 0.0646 - val_loss: 0.0011 - val_mae: 0.0274\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0049 - mae: 0.0534 - val_loss: 0.0016 - val_mae: 0.0354\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0059 - mae: 0.0580 - val_loss: 0.0018 - val_mae: 0.0383\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0064 - mae: 0.0607 - val_loss: 0.0012 - val_mae: 0.0303\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0052 - mae: 0.0573 - val_loss: 9.2894e-04 - val_mae: 0.0245\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0067 - mae: 0.0563 - val_loss: 9.7550e-04 - val_mae: 0.0256\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0040 - mae: 0.0470 - val_loss: 0.0013 - val_mae: 0.0326\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0045 - mae: 0.0518 - val_loss: 0.0016 - val_mae: 0.0372\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0046 - mae: 0.0542 - val_loss: 0.0014 - val_mae: 0.0339\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0047 - mae: 0.0526 - val_loss: 0.0013 - val_mae: 0.0327\n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0041 - mae: 0.0479 - val_loss: 0.0016 - val_mae: 0.0359\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0048 - mae: 0.0496 - val_loss: 0.0016 - val_mae: 0.0354\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0055 - mae: 0.0570 - val_loss: 0.0013 - val_mae: 0.0311\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0045 - mae: 0.0527 - val_loss: 0.0013 - val_mae: 0.0311\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0046 - mae: 0.0505 - val_loss: 0.0014 - val_mae: 0.0337\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0037 - mae: 0.0460 - val_loss: 0.0013 - val_mae: 0.0318\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0039 - mae: 0.0477 - val_loss: 0.0011 - val_mae: 0.0278\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0049 - mae: 0.0491 - val_loss: 0.0010 - val_mae: 0.0256\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0047 - mae: 0.0490 - val_loss: 0.0012 - val_mae: 0.0295\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0047 - mae: 0.0500 - val_loss: 0.0014 - val_mae: 0.0339\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0042 - mae: 0.0470 - val_loss: 0.0013 - val_mae: 0.0331\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0043 - mae: 0.0490 - val_loss: 0.0012 - val_mae: 0.0315\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0042 - mae: 0.0490 - val_loss: 0.0015 - val_mae: 0.0352\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0043 - mae: 0.0483 - val_loss: 0.0011 - val_mae: 0.0293\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0042 - mae: 0.0475 - val_loss: 9.5073e-04 - val_mae: 0.0258\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0033 - mae: 0.0441 - val_loss: 9.1782e-04 - val_mae: 0.0253\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0036 - mae: 0.0459 - val_loss: 0.0012 - val_mae: 0.0302\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.0040 - mae: 0.0475 - val_loss: 0.0013 - val_mae: 0.0324\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0050 - mae: 0.0538 - val_loss: 0.0010 - val_mae: 0.0279\n"
     ]
    }
   ],
   "source": [
    "# Data is already in correct shape for LSTM: (samples, timesteps, features)\n",
    "print(f\"X_train_scaled_ts shape: {X_train_scaled_ts.shape}\")\n",
    "print(f\"X_test_scaled_ts shape: {X_test_scaled_ts.shape}\")\n",
    "\n",
    "# Build Time Series LSTM Model\n",
    "model_ts = Sequential()\n",
    "model_ts.add(LSTM(units=128, return_sequences=True, input_shape=(X_train_scaled_ts.shape[1], X_train_scaled_ts.shape[2])))\n",
    "model_ts.add(Dropout(0.2))\n",
    "model_ts.add(LSTM(units=64, return_sequences=False))\n",
    "model_ts.add(Dropout(0.2))\n",
    "model_ts.add(Dense(32, activation='relu'))\n",
    "model_ts.add(Dense(1))\n",
    "\n",
    "model_ts.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model_ts.summary()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining the model...\")\n",
    "history_ts = model_ts.fit(\n",
    "    X_train_scaled_ts, \n",
    "    y_train_scaled_ts, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870fe91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "=== Time Series LSTM Results ===\n",
      "\n",
      "Scaled Data Metrics:\n",
      "Mean Squared Error: 0.003470\n",
      "Mean Absolute Error: 0.042263\n",
      "R² Score: 0.955387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions_ts = model_ts.predict(X_test_scaled_ts)\n",
    "\n",
    "# Calculate metrics on scaled data\n",
    "mse_scaled = mean_squared_error(y_test_scaled_ts, predictions_ts.flatten())\n",
    "mae_scaled = mean_absolute_error(y_test_scaled_ts, predictions_ts.flatten())\n",
    "r2_scaled = r2_score(y_test_scaled_ts, predictions_ts.flatten())\n",
    "\n",
    "\n",
    "print(\"=== Time Series LSTM Results ===\")\n",
    "print(\"\\nScaled Data Metrics:\")\n",
    "print(f\"Mean Squared Error: {mse_scaled:.6f}\")\n",
    "print(f\"Mean Absolute Error: {mae_scaled:.6f}\")\n",
    "print(f\"R² Score: {r2_scaled:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e49aaae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hyperparameter Tuning ===\n",
      "\n",
      "Testing combination 1/5: {'lstm_units': 128, 'dropout': 0.2, 'batch_size': 32, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.942668\n",
      "\n",
      "Testing combination 2/5: {'lstm_units': 256, 'dropout': 0.1, 'batch_size': 32, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d127c8ae160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "R² Score: 0.917339\n",
      "\n",
      "Testing combination 3/5: {'lstm_units': 64, 'dropout': 0.3, 'batch_size': 16, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7d123829a340> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "R² Score: 0.949458\n",
      "\n",
      "Testing combination 4/5: {'lstm_units': 128, 'dropout': 0.1, 'batch_size': 64, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: -0.145263\n",
      "\n",
      "Testing combination 5/5: {'lstm_units': 256, 'dropout': 0.2, 'batch_size': 16, 'lr': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashwin/Desktop/Internship/code/Indonesia-rice-prediction/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² Score: 0.929776\n",
      "\n",
      "=== Best Hyperparameters Found ===\n",
      "Best parameters: {'lstm_units': 64, 'dropout': 0.3, 'batch_size': 16, 'lr': 0.01}\n",
      "Best R² Score: 0.949458\n",
      "\n",
      "=== Final Tuned Model Results ===\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
      "\n",
      "Best Model - Scaled Data Metrics:\n",
      "Mean Squared Error: 0.003932\n",
      "Mean Absolute Error: 0.043451\n",
      "R² Score: 0.949458\n"
     ]
    }
   ],
   "source": [
    "# 6th cell: Hyperparameter tuning for maximum accuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import itertools\n",
    "\n",
    "print(\"=== Hyperparameter Tuning ===\")\n",
    "\n",
    "# Define hyperparameter combinations to try\n",
    "lstm_units_options = [64, 128, 256]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "batch_sizes = [16, 32, 64]\n",
    "learning_rates = [0.001, 0.01]\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Grid search through hyperparameters (simplified for computational efficiency)\n",
    "param_combinations = [\n",
    "    {'lstm_units': 128, 'dropout': 0.2, 'batch_size': 32, 'lr': 0.001},\n",
    "    {'lstm_units': 256, 'dropout': 0.1, 'batch_size': 32, 'lr': 0.001},\n",
    "    {'lstm_units': 64, 'dropout': 0.3, 'batch_size': 16, 'lr': 0.01},\n",
    "    {'lstm_units': 128, 'dropout': 0.1, 'batch_size': 64, 'lr': 0.001},\n",
    "    {'lstm_units': 256, 'dropout': 0.2, 'batch_size': 16, 'lr': 0.01}\n",
    "]\n",
    "\n",
    "for i, params in enumerate(param_combinations):\n",
    "    print(f\"\\nTesting combination {i+1}/{len(param_combinations)}: {params}\")\n",
    "    \n",
    "    # Build model with current hyperparameters\n",
    "    model_tuned = Sequential()\n",
    "    model_tuned.add(LSTM(units=params['lstm_units'], return_sequences=True, \n",
    "                        input_shape=(X_train_scaled_ts.shape[1], X_train_scaled_ts.shape[2])))\n",
    "    model_tuned.add(Dropout(params['dropout']))\n",
    "    model_tuned.add(LSTM(units=params['lstm_units']//2, return_sequences=False))\n",
    "    model_tuned.add(Dropout(params['dropout']))\n",
    "    model_tuned.add(Dense(32, activation='relu'))\n",
    "    model_tuned.add(Dense(1))\n",
    "    \n",
    "    # Compile with current learning rate\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    optimizer = Adam(learning_rate=params['lr'])\n",
    "    model_tuned.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "    \n",
    "    # Train model\n",
    "    history_tuned = model_tuned.fit(\n",
    "        X_train_scaled_ts, \n",
    "        y_train_scaled_ts,\n",
    "        epochs=50,  # Reduced for tuning efficiency\n",
    "        batch_size=params['batch_size'],\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    predictions_tuned = model_tuned.predict(X_test_scaled_ts, verbose=0)\n",
    "    r2_tuned = r2_score(y_test_scaled_ts, predictions_tuned.flatten())\n",
    "    \n",
    "    print(f\"R² Score: {r2_tuned:.6f}\")\n",
    "    \n",
    "    # Update best model if this one is better\n",
    "    if r2_tuned > best_r2:\n",
    "        best_r2 = r2_tuned\n",
    "        best_params = params.copy()\n",
    "        best_model = model_tuned\n",
    "        \n",
    "print(f\"\\n=== Best Hyperparameters Found ===\")\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best R² Score: {best_r2:.6f}\")\n",
    "\n",
    "# Final evaluation with best model\n",
    "print(f\"\\n=== Final Tuned Model Results ===\")\n",
    "best_predictions = best_model.predict(X_test_scaled_ts)\n",
    "\n",
    "# Metrics on scaled data\n",
    "best_mse_scaled = mean_squared_error(y_test_scaled_ts, best_predictions.flatten())\n",
    "best_mae_scaled = mean_absolute_error(y_test_scaled_ts, best_predictions.flatten())\n",
    "best_r2_scaled = r2_score(y_test_scaled_ts, best_predictions.flatten())\n",
    "\n",
    "print(\"\\nBest Model - Scaled Data Metrics:\")\n",
    "print(f\"Mean Squared Error: {best_mse_scaled:.6f}\")\n",
    "print(f\"Mean Absolute Error: {best_mae_scaled:.6f}\")\n",
    "print(f\"R² Score: {best_r2_scaled:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
